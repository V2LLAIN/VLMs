{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ\n",
    "\n",
    "#### 1ë‹¨ê³„: FTë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ:\n",
    "\n",
    "\n",
    "`bitsandbytes`:  ğŸ¤—ì˜ CUDA custom functions: íŠ¹íˆë‚˜ 8-bit optimizers ë° quantization functionsë¥¼ ë‘˜ëŸ¬ì‹¸ëŠ” lightweight wrapper. QLoRAì˜ quantization processì²˜ë¦¬ë¥¼ ìœ„í•´ ì‚¬ìš©.\n",
    "\n",
    "`peft`: ğŸ¤—ì˜ FT ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "\n",
    "`transformers`: ğŸ¤—ì˜ PLMs ë° training utilites ì œê³µ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "\n",
    "`accelerate`: ğŸ¤—ì˜ multi-GPUs/TPU/fp16 ê´€ë ¨ ì½”ë“œ ì¶”ìƒí™”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬.\n",
    "\n",
    "`loralib`: LoRAì˜ Pytorch êµ¬í˜„ ë¼ì´ë¸ŒëŸ¬ë¦¬.\n",
    "\n",
    "`einops`: í…ì„œ ì—°ì‚° ë‹¨ìˆœí™” ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "\n",
    "`xformers`: êµ¬ì„±ê°€ëŠ¥í•œ transformer building blocks collection ë¼ì´ë¸ŒëŸ¬ë¦¬."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes transformers datasets accelerate loralib einops xformers\n",
    "!pip install -U git+https://github.com/huggingface/peft.git\n",
    "\n",
    "import os\n",
    "import bitsandbytes as bnb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel, \n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    set_seed,\n",
    "    pipeline,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLMs ë¡œë“œ (ğŸŒŸğŸŒŸğŸŒŸ)\n",
    "\n",
    "`AutoModelForCausalLM.from_pretrained()`í•¨ìˆ˜ë¥¼ ì´ìš©í•´ ğŸ¤—transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ê°€ì ¸ì˜´.\n",
    "\n",
    "ì´ë•Œ, ëª¨ë¸ì€ `BitsAndBytesConfig`ë¥¼ ì´ìš©í•´ 4-bitë¡œ ë¡œë“œë˜ëŠ”ë°, ì´ëŠ” ëª¨ë¸ì˜ ì‚¬ì „ê°€ì¤‘ì¹˜ë¥¼ 4bitë¡œ ì–‘ìí™”í•˜ê³ , FTì¤‘ ê³ ì •í•˜ëŠ” ê²ƒì„ í¬í•¨í•˜ëŠ” `Q-LoRAê³¼ì •ì˜ ì¼ë¶€`ì´ë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"tiiuae/falcon-7b\"\n",
    "\n",
    "# Setting Global Parameters\n",
    "\n",
    "# 'new_model' is the name that you want to give to the fine-tuned model.\n",
    "new_model = \"new-model-name\"\n",
    "username = \"chan4im\"\n",
    "# 'hf_model_repo' is the identifier for the Hugging Face repository where you want to save the fine-tuned model.\n",
    "hf_model_repo=f\"{username}/\"+new_model\n",
    "\n",
    "\n",
    "\n",
    "# Load Model on GPU \n",
    "# 'device_map' is a dictionary that maps devices to model parts. In this case, it is set to {\"\": 0}, which means that the entire model will be loaded on GPU 0.\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    load_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRAë¥¼ ìœ„í•œ ëª¨ë¸ ì¤€ë¹„\n",
    "\n",
    "`prepare_model_for_kbit_training()`í•¨ìˆ˜ë¡œ QLoRAì— ëŒ€í•œ ì¤€ë¹„ ì§„í–‰:\n",
    "\n",
    "ì´ í•¨ìˆ˜ëŠ” í•„ìš”í•œ configurationì„ ì„¤ì •, QLoRAì— ëŒ€í•œ ëª¨ë¸ì„ ì´ˆê¸°í™”í•¨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA Configuring\n",
    "\n",
    "LoRAì— ëŒ€í•œ êµ¬ì„±ì€ `LoraConfig`í´ë˜ìŠ¤ë¡œ ì„¤ì •í•˜ë©°, ì´ í´ë˜ìŠ¤ì˜ ë§¤ê°œë³€ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ë‹¤:\n",
    "\n",
    "`r`: updateí–‰ë ¬ì˜ rank (ë‚®ì„ìˆ˜ë¡ í›ˆë ¨ê°€ëŠ¥ parameterê°€ ì¤„ì–´ë“ ë‹¤.)\n",
    "\n",
    "`lora_alpha`: LoRA scalingì¸ì (LoRAì¸µ ì¶œë ¥ì— ê³±í•´ì§€ëŠ” ê°’.)\n",
    "\n",
    "`target_modules`: LoRA updateí–‰ë ¬ ì ìš©í•  ëª¨ë“ˆë“¤. ex) attention blocksê°™ì€ ëª¨ë¸ì˜ íŠ¹ì •ë¶€ë¶„ì— LoRAì ìš©ê°€ëŠ¥.\n",
    "\n",
    "`lora_dropout`: LoRA Layerì˜ Dropoutë¥ .\n",
    "\n",
    "`bias`: bias parameterë¥¼ trainí• ê±´ì§€ì˜ ì—¬ë¶€. (Can be â€˜noneâ€™, â€˜allâ€™ or â€˜lora_onlyâ€™).\n",
    "\n",
    "##### ëª¨ë¸ì€ LoRA Configuration ì„¤ì •ì„ í†µí•´ `get_peft_model()`í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ updateí•œë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading & Preparing Data\n",
    "\n",
    "`load_dataset()`í•¨ìˆ˜ë¡œ ğŸ¤— datasetsë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ dataë¡œë“œ.\n",
    "\n",
    "Dataset shuffled, `generate_and_tokenize_prompt()`í•¨ìˆ˜ì— ë§µí•‘.\n",
    "\n",
    "`generate_and_tokenize_prompt()`í•¨ìˆ˜ëŠ” ê° datasetì˜ ê° datapointë¥¼ ìƒì„±í•˜ê³  tokenizeí•¨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "  return f\"\"\"<Human>: {data_point[\"Context\"]}\n",
    "                <AI>: {data_point[\"Response\"]}\"\"\".strip()\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "  full_prompt = generate_prompt(data_point)\n",
    "  tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n",
    "  return tokenized_full_prompt\n",
    "\n",
    "\n",
    "dataset_name = 'Amod/mental_health_counseling_conversations'\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "dataset = dataset.shuffle().map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Argument Setting\n",
    "\n",
    "trainingì¸ìëŠ” transformersë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ `TrainingArguments`í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•´ ì„¤ì •ëœë‹¤:\n",
    "\n",
    "- `auto_find_batch_size`: Trueë¡œ ì„¤ì •í•˜ë©´ íŠ¸ë ˆì´ë„ˆê°€ ë©”ëª¨ë¦¬ì— ë§ëŠ” ê°€ì¥ í° ë°°ì¹˜ í¬ê¸°ë¥¼ ìë™ìœ¼ë¡œ ì°¾ëŠ”ë‹¤.\n",
    "- `num_train_epochs`: í•™ìŠµ epochì˜ ìˆ˜ì…ë‹ˆë‹¤.\n",
    "- `learning_rate`: optimizerì˜ í•™ìŠµë¥ \n",
    "- `bf16`: Trueë¡œ ì„¤ì •í•˜ë©´ íŠ¸ë ˆì´ë„ˆê°€ í•™ìŠµì— bf16(Brain Floating Point) ì •ë°€ë„ë¥¼ ì‚¬ìš©í•¨. (ë©”ëª¨ë¦¬ì ˆì•½&í›ˆë ¨ ì†ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒ)\n",
    "- `save_total_limit`: ì €ì¥í•  ìˆ˜ ìˆëŠ” ì´ ckptì˜ ìˆ˜ì…ë‹ˆë‹¤.\n",
    "- `logging_steps`: ê° ë¡œê¹… ì‚¬ì´ì˜ stepìˆ˜.\n",
    "- `output_dir`: ëª¨ë¸ ckptê°€ ì €ì¥ë  ë””ë ‰í† ë¦¬.\n",
    "- `save_strategy`: ckptì €ì¥ì„ ìœ„í•œ ì „ëµ. ì•„ë˜ ì˜ˆì‹œì˜ ê²½ìš°, ê° epoch ì¢…ë£Œ í›„ ì €ì¥ë¨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    auto_find_batch_size=True,\n",
    "    num_train_epochs=4,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True, \n",
    "    save_total_limit=4,\n",
    "    logging_steps=10,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    save_strategy='epoch',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training!\n",
    "\n",
    "ğŸ¤—ì˜ transformersë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ `Trainer`í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•´ í•™ìŠµí•œë‹¤.\n",
    "\n",
    "`ëª¨ë¸ëª…`, `train_dataset`, `training_args`, `ì–¸ì–´ëª¨ë¸ë§ì„ ìœ„í•œ data_collator`ë¥¼ ê°€ì ¸ì˜¨ë‹¤.\n",
    "\n",
    "ì´í›„ `train()`í•¨ìˆ˜ë¡œ í›ˆë ¨ì„ ì‹œì‘í•œë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset['valid'],\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=512,\n",
    "        tokenizer=tokenizer,\n",
    "        args=args,\n",
    ")\n",
    "trainer.train()\n",
    "# save model in local\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After Training?\n",
    "\n",
    "í•™ìŠµì´ ì™„ë£Œëœ ì´í›„, \n",
    "\n",
    "1. ë¡œì»¬ì— ì €ì¥ or ğŸ¤—ì— ì—…ë¡œë“œí•´ ğŸ¤— PEFTì™€ í•¨ê»˜ ì‚¬ìš©ê°€ëŠ¥í•˜ë‹¤. \n",
    "\n",
    "2. ğŸ¤— PEFTë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ `model.merge_and_unload()`í•¨ìˆ˜ë¥¼ ì‚¬ìš©, LoRAë¥¼ foundation-LLMê³¼ í•©ì¹  ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë°©ë²• 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"path_to_save_model\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "huggingface-cli login\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n",
    "\n",
    "model.push_to_hub(\"your_model_name\")\n",
    "tokenizer.push_to_hub(\"your_model_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë°©ë²• 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login, login\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "notebook_login()\n",
    "load_dotenv()\n",
    "login(token=os.getenv(\"HF_HUB_TOKEN\"))\n",
    "hf_adapter_repo=f\"{username}/{model_name}\"\n",
    "     \n",
    "\n",
    "# 'trainer.push_to_hub(hf_adapter_repo)' is a method that pushes the trained model adapter to the Hugging Face Model Hub. \n",
    "# 'hf_adapter_repo' is the repository name for the model adapter on the Hugging Face Model Hub.\n",
    "\n",
    "\n",
    "# Save the adapter\n",
    "trainer.push_to_hub(hf_adapter_repo)\n",
    "     \n",
    "\n",
    "# 'del model' and 'del trainer' are lines of code that delete the 'model' and 'trainer' objects. This frees up the memory that was used by these objects.\n",
    "# 'import gc' is a line of code that imports the 'gc' module, which provides an interface to the garbage collector.\n",
    "# 'gc.collect()' is a function that triggers a full garbage collection. It frees up memory by collecting all the objects that are no longer in use.\n",
    "# This block of code is used to empty the VRAM (Video Random Access Memory) by deleting the 'model' and 'trainer' objects and then triggering a full garbage collection.\n",
    "# Empty VRAM\n",
    "del model\n",
    "del trainer\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "     \n",
    "\n",
    "# 'torch.cuda.empty_cache()' is a function from the PyTorch library that releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.\n",
    "# It's a PyTorch specific function to manage GPU memory and it doesn't affect the GPU memory usage by PyTorch tensors.\n",
    "# This line of code is used to empty the cache memory that's used by PyTorch on the GPU.\n",
    "torch.cuda.empty_cache() # PyTorch thing\n",
    "     \n",
    "\n",
    "# 'torch.cuda.empty_cache()' is a function from the PyTorch library that releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.\n",
    "# It's a PyTorch specific function to manage GPU memory and it doesn't affect the GPU memory usage by PyTorch tensors.\n",
    "# This line of code is used to empty the cache memory that's used by PyTorch on the GPU.\n",
    "torch.cuda.empty_cache() # PyTorch thing\n",
    "     \n",
    "\n",
    "# 'hf_adapter_repo' is a variable that holds the repository name for the Hugging Face model adapter.\n",
    "# 'edumunozsala/phi-3-mini-QLoRA' is the repository name, where 'edumunozsala' is the username of the repository owner and 'phi-3-mini-QLoRA' is the name of the model adapter.\n",
    "# 'model_name, hf_adapter_repo, compute_dtype' is a line of code that returns the values of the 'model_name', 'hf_adapter_repo', and 'compute_dtype' variables.\n",
    "# This block of code is used to set the repository name for the Hugging Face model adapter and then return the values of the 'model_name', 'hf_adapter_repo', and 'compute_dtype' variables.\n",
    "#hf_adapter_repo = \"edumunozsala/phi-3-mini-QLoRA\"\n",
    "\n",
    "\n",
    "\n",
    "# 'peft_model_id' and 'tr_model_id' are variables that hold the identifiers for the PEFT model and the transformer model, respectively.\n",
    "# 'AutoModelForCausalLM.from_pretrained(tr_model_id, trust_remote_code=True, torch_dtype=compute_dtype)' is a function that loads a pre-trained transformer model for causal language modeling. 'tr_model_id' is the identifier for the pre-trained model, 'trust_remote_code=True' allows the execution of code from the model file, and 'torch_dtype=compute_dtype' sets the data type for the PyTorch tensors.\n",
    "# 'PeftModel.from_pretrained(model, peft_model_id)' is a function that loads a pre-trained PEFT model. 'model' is the transformer model and 'peft_model_id' is the identifier for the pre-trained PEFT model.\n",
    "# 'model.merge_and_unload()' is a method that merges the PEFT model with the transformer model and then unloads the PEFT model.\n",
    "# This block of code is used to load a pre-trained transformer model and a pre-trained PEFT model, merge the two models, and then unload the PEFT model.\n",
    "peft_model_id = hf_adapter_repo\n",
    "tr_model_id = model_name\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(tr_model_id, trust_remote_code=True, torch_dtype=compute_dtype)\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "model = model.merge_and_unload()\n",
    "     \n",
    "\n",
    "# 'tokenizer' is a variable that holds the tokenizer.\n",
    "\n",
    "# 'AutoTokenizer.from_pretrained(peft_model_id)' is a function from the Hugging Face Transformers library that loads a pre-trained tokenizer. 'peft_model_id' is the identifier for the pre-trained tokenizer.\n",
    "\n",
    "# This line of code is used to load a pre-trained tokenizer.\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
    "     \n",
    "\n",
    "# 'hf_model_repo' is a variable that holds the repository name for the Hugging Face model.\n",
    "\n",
    "# This line of code is used to reference the repository name for the Hugging Face model.\n",
    "hf_model_repo\n",
    "     \n",
    "\n",
    "# 'merged_model_id' is a variable that holds the identifier for the merged model.\n",
    "\n",
    "# 'hf_model_repo' is the repository name for the Hugging Face model.\n",
    "\n",
    "# 'model.push_to_hub(merged_model_id)' is a method that pushes the merged model to the Hugging Face Model Hub. 'merged_model_id' is the identifier for the merged model.\n",
    "\n",
    "# 'tokenizer.push_to_hub(merged_model_id)' is a method that pushes the tokenizer to the Hugging Face Model Hub. 'merged_model_id' is the identifier for the tokenizer.\n",
    "\n",
    "# This block of code is used to save the merged model and the tokenizer to the Hugging Face Model Hub.\n",
    "# SAve the model merged to the Hub\n",
    "merged_model_id = hf_model_repo\n",
    "model.push_to_hub(merged_model_id)\n",
    "tokenizer.push_to_hub(merged_model_id)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
