{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 필요 라이브러리 로드\n",
    "\n",
    "#### 1단계: FT를 위한 라이브러리 로드:\n",
    "\n",
    "\n",
    "`bitsandbytes`:  🤗의 CUDA custom functions: 특히나 8-bit optimizers 및 quantization functions를 둘러싸는 lightweight wrapper. QLoRA의 quantization process처리를 위해 사용.\n",
    "\n",
    "`peft`: 🤗의 FT 라이브러리\n",
    "\n",
    "`transformers`: 🤗의 PLMs 및 training utilites 제공 라이브러리\n",
    "\n",
    "`accelerate`: 🤗의 multi-GPUs/TPU/fp16 관련 코드 추상화한 라이브러리.\n",
    "\n",
    "`loralib`: LoRA의 Pytorch 구현 라이브러리.\n",
    "\n",
    "`einops`: 텐서 연산 단순화 라이브러리\n",
    "\n",
    "`xformers`: 구성가능한 transformer building blocks collection 라이브러리."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes transformers datasets accelerate loralib einops xformers\n",
    "!pip install -U git+https://github.com/huggingface/peft.git\n",
    "\n",
    "import os\n",
    "import bitsandbytes as bnb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel, \n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    set_seed,\n",
    "    pipeline,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLMs 로드 (🌟🌟🌟)\n",
    "\n",
    "`AutoModelForCausalLM.from_pretrained()`함수를 이용해 🤗transformers 라이브러리에서 가져옴.\n",
    "\n",
    "이때, 모델은 `BitsAndBytesConfig`를 이용해 4-bit로 로드되는데, 이는 모델의 사전가중치를 4bit로 양자화하고, FT중 고정하는 것을 포함하는 `Q-LoRA과정의 일부`이다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"tiiuae/falcon-7b\"\n",
    "\n",
    "# Setting Global Parameters\n",
    "\n",
    "# 'new_model' is the name that you want to give to the fine-tuned model.\n",
    "new_model = \"new-model-name\"\n",
    "username = \"chan4im\"\n",
    "# 'hf_model_repo' is the identifier for the Hugging Face repository where you want to save the fine-tuned model.\n",
    "hf_model_repo=f\"{username}/\"+new_model\n",
    "\n",
    "\n",
    "\n",
    "# Load Model on GPU \n",
    "# 'device_map' is a dictionary that maps devices to model parts. In this case, it is set to {\"\": 0}, which means that the entire model will be loaded on GPU 0.\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    load_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRA를 위한 모델 준비\n",
    "\n",
    "`prepare_model_for_kbit_training()`함수로 QLoRA에 대한 준비 진행:\n",
    "\n",
    "이 함수는 필요한 configuration을 설정, QLoRA에 대한 모델을 초기화함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA Configuring\n",
    "\n",
    "LoRA에 대한 구성은 `LoraConfig`클래스로 설정하며, 이 클래스의 매개변수는 아래와 같다:\n",
    "\n",
    "`r`: update행렬의 rank (낮을수록 훈련가능 parameter가 줄어든다.)\n",
    "\n",
    "`lora_alpha`: LoRA scaling인자 (LoRA층 출력에 곱해지는 값.)\n",
    "\n",
    "`target_modules`: LoRA update행렬 적용할 모듈들. ex) attention blocks같은 모델의 특정부분에 LoRA적용가능.\n",
    "\n",
    "`lora_dropout`: LoRA Layer의 Dropout률.\n",
    "\n",
    "`bias`: bias parameter를 train할건지의 여부. (Can be ‘none’, ‘all’ or ‘lora_only’).\n",
    "\n",
    "##### 모델은 LoRA Configuration 설정을 통해 `get_peft_model()`함수를 사용해 update한다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading & Preparing Data\n",
    "\n",
    "`load_dataset()`함수로 🤗 datasets라이브러리에서 data로드.\n",
    "\n",
    "Dataset shuffled, `generate_and_tokenize_prompt()`함수에 맵핑.\n",
    "\n",
    "`generate_and_tokenize_prompt()`함수는 각 dataset의 각 datapoint를 생성하고 tokenize함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "  return f\"\"\"<Human>: {data_point[\"Context\"]}\n",
    "                <AI>: {data_point[\"Response\"]}\"\"\".strip()\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "  full_prompt = generate_prompt(data_point)\n",
    "  tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n",
    "  return tokenized_full_prompt\n",
    "\n",
    "\n",
    "dataset_name = 'Amod/mental_health_counseling_conversations'\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "dataset = dataset.shuffle().map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Argument Setting\n",
    "\n",
    "training인자는 transformers라이브러리의 `TrainingArguments`클래스를 사용해 설정된다:\n",
    "\n",
    "- `auto_find_batch_size`: True로 설정하면 트레이너가 메모리에 맞는 가장 큰 배치 크기를 자동으로 찾는다.\n",
    "- `num_train_epochs`: 학습 epoch의 수입니다.\n",
    "- `learning_rate`: optimizer의 학습률\n",
    "- `bf16`: True로 설정하면 트레이너가 학습에 bf16(Brain Floating Point) 정밀도를 사용함. (메모리절약&훈련 속도를 크게 향상시킬 수 있음)\n",
    "- `save_total_limit`: 저장할 수 있는 총 ckpt의 수입니다.\n",
    "- `logging_steps`: 각 로깅 사이의 step수.\n",
    "- `output_dir`: 모델 ckpt가 저장될 디렉토리.\n",
    "- `save_strategy`: ckpt저장을 위한 전략. 아래 예시의 경우, 각 epoch 종료 후 저장됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    auto_find_batch_size=True,\n",
    "    num_train_epochs=4,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True, \n",
    "    save_total_limit=4,\n",
    "    logging_steps=10,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    save_strategy='epoch',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training!\n",
    "\n",
    "🤗의 transformers라이브러리의 `Trainer`클래스를 사용해 학습한다.\n",
    "\n",
    "`모델명`, `train_dataset`, `training_args`, `언어모델링을 위한 data_collator`를 가져온다.\n",
    "\n",
    "이후 `train()`함수로 훈련을 시작한다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset['valid'],\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=512,\n",
    "        tokenizer=tokenizer,\n",
    "        args=args,\n",
    ")\n",
    "trainer.train()\n",
    "# save model in local\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After Training?\n",
    "\n",
    "학습이 완료된 이후, \n",
    "\n",
    "1. 로컬에 저장 or 🤗에 업로드해 🤗 PEFT와 함께 사용가능하다. \n",
    "\n",
    "2. 🤗 PEFT라이브러리의 `model.merge_and_unload()`함수를 사용, LoRA를 foundation-LLM과 합칠 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 방법 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"path_to_save_model\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "huggingface-cli login\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n",
    "\n",
    "model.push_to_hub(\"your_model_name\")\n",
    "tokenizer.push_to_hub(\"your_model_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 방법 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login, login\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "notebook_login()\n",
    "load_dotenv()\n",
    "login(token=os.getenv(\"HF_HUB_TOKEN\"))\n",
    "hf_adapter_repo=f\"{username}/{model_name}\"\n",
    "     \n",
    "\n",
    "# 'trainer.push_to_hub(hf_adapter_repo)' is a method that pushes the trained model adapter to the Hugging Face Model Hub. \n",
    "# 'hf_adapter_repo' is the repository name for the model adapter on the Hugging Face Model Hub.\n",
    "\n",
    "\n",
    "# Save the adapter\n",
    "trainer.push_to_hub(hf_adapter_repo)\n",
    "     \n",
    "\n",
    "# 'del model' and 'del trainer' are lines of code that delete the 'model' and 'trainer' objects. This frees up the memory that was used by these objects.\n",
    "# 'import gc' is a line of code that imports the 'gc' module, which provides an interface to the garbage collector.\n",
    "# 'gc.collect()' is a function that triggers a full garbage collection. It frees up memory by collecting all the objects that are no longer in use.\n",
    "# This block of code is used to empty the VRAM (Video Random Access Memory) by deleting the 'model' and 'trainer' objects and then triggering a full garbage collection.\n",
    "# Empty VRAM\n",
    "del model\n",
    "del trainer\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "     \n",
    "\n",
    "# 'torch.cuda.empty_cache()' is a function from the PyTorch library that releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.\n",
    "# It's a PyTorch specific function to manage GPU memory and it doesn't affect the GPU memory usage by PyTorch tensors.\n",
    "# This line of code is used to empty the cache memory that's used by PyTorch on the GPU.\n",
    "torch.cuda.empty_cache() # PyTorch thing\n",
    "     \n",
    "\n",
    "# 'torch.cuda.empty_cache()' is a function from the PyTorch library that releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.\n",
    "# It's a PyTorch specific function to manage GPU memory and it doesn't affect the GPU memory usage by PyTorch tensors.\n",
    "# This line of code is used to empty the cache memory that's used by PyTorch on the GPU.\n",
    "torch.cuda.empty_cache() # PyTorch thing\n",
    "     \n",
    "\n",
    "# 'hf_adapter_repo' is a variable that holds the repository name for the Hugging Face model adapter.\n",
    "# 'edumunozsala/phi-3-mini-QLoRA' is the repository name, where 'edumunozsala' is the username of the repository owner and 'phi-3-mini-QLoRA' is the name of the model adapter.\n",
    "# 'model_name, hf_adapter_repo, compute_dtype' is a line of code that returns the values of the 'model_name', 'hf_adapter_repo', and 'compute_dtype' variables.\n",
    "# This block of code is used to set the repository name for the Hugging Face model adapter and then return the values of the 'model_name', 'hf_adapter_repo', and 'compute_dtype' variables.\n",
    "#hf_adapter_repo = \"edumunozsala/phi-3-mini-QLoRA\"\n",
    "\n",
    "\n",
    "\n",
    "# 'peft_model_id' and 'tr_model_id' are variables that hold the identifiers for the PEFT model and the transformer model, respectively.\n",
    "# 'AutoModelForCausalLM.from_pretrained(tr_model_id, trust_remote_code=True, torch_dtype=compute_dtype)' is a function that loads a pre-trained transformer model for causal language modeling. 'tr_model_id' is the identifier for the pre-trained model, 'trust_remote_code=True' allows the execution of code from the model file, and 'torch_dtype=compute_dtype' sets the data type for the PyTorch tensors.\n",
    "# 'PeftModel.from_pretrained(model, peft_model_id)' is a function that loads a pre-trained PEFT model. 'model' is the transformer model and 'peft_model_id' is the identifier for the pre-trained PEFT model.\n",
    "# 'model.merge_and_unload()' is a method that merges the PEFT model with the transformer model and then unloads the PEFT model.\n",
    "# This block of code is used to load a pre-trained transformer model and a pre-trained PEFT model, merge the two models, and then unload the PEFT model.\n",
    "peft_model_id = hf_adapter_repo\n",
    "tr_model_id = model_name\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(tr_model_id, trust_remote_code=True, torch_dtype=compute_dtype)\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "model = model.merge_and_unload()\n",
    "     \n",
    "\n",
    "# 'tokenizer' is a variable that holds the tokenizer.\n",
    "\n",
    "# 'AutoTokenizer.from_pretrained(peft_model_id)' is a function from the Hugging Face Transformers library that loads a pre-trained tokenizer. 'peft_model_id' is the identifier for the pre-trained tokenizer.\n",
    "\n",
    "# This line of code is used to load a pre-trained tokenizer.\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
    "     \n",
    "\n",
    "# 'hf_model_repo' is a variable that holds the repository name for the Hugging Face model.\n",
    "\n",
    "# This line of code is used to reference the repository name for the Hugging Face model.\n",
    "hf_model_repo\n",
    "     \n",
    "\n",
    "# 'merged_model_id' is a variable that holds the identifier for the merged model.\n",
    "\n",
    "# 'hf_model_repo' is the repository name for the Hugging Face model.\n",
    "\n",
    "# 'model.push_to_hub(merged_model_id)' is a method that pushes the merged model to the Hugging Face Model Hub. 'merged_model_id' is the identifier for the merged model.\n",
    "\n",
    "# 'tokenizer.push_to_hub(merged_model_id)' is a method that pushes the tokenizer to the Hugging Face Model Hub. 'merged_model_id' is the identifier for the tokenizer.\n",
    "\n",
    "# This block of code is used to save the merged model and the tokenizer to the Hugging Face Model Hub.\n",
    "# SAve the model merged to the Hub\n",
    "merged_model_id = hf_model_repo\n",
    "model.push_to_hub(merged_model_id)\n",
    "tokenizer.push_to_hub(merged_model_id)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
